#' LASSO Regression for Multiply Imputed Datasets using a Grouped Penalty
#' 
#' \code{impgrplasso} provides functionality for performing least absolute 
#' shrinkage and selection operator (LASSO) regression on multiply imputed data. 
#' This code combines the group LASSO functionality of the 'grplasso' package 
#' (Meier et al. 2008) with the concepts of the multiple imputation LASSO regression 
#' (MI-LASSO) from Chen and Wang 2013. The concept behind MI-LASSO is that one 
#' can "stack" the imputed datasets and then use a group LASSO penalty to 
#' calculate shared penalties across all imputed versions of the same variable. 
#' Grouped penalties are commonly used for categorical variables with more than 
#' two categories so that the dummy variables resulting from that variable all
#' experience the same penalization, and this grouping of dummy variables 
#' originating from the same categorical variable is automatically done in the
#' \code{impgrplasso} function. 
#' 
#' One criticism from a review on available methods for multiply imputed LASSO 
#' regression from Gunn et al. 2023 is that the Chen and Wang 2013 MI-LASSO 
#' method should have the optimal lambda value chosen based on cross-validation 
#' and MSE rather than the BIC values Chen and Wang used since the latter is 
#' based on degrees of freedom that are difficult to calculate for LASSO 
#' regression. This function adopts the use of cross validation and an MSE or 
#' log-loss loss function for continuous dependent variables and binary dependent
#' variables (i.e., for LASSO logistic regression), respectively. If the function
#' is provided a vector of lambda values, this cross validation and loss 
#' calculation process is performed to identify an optimal lambda value. Once 
#' identified, providing the function a single optimal lambda value will perform
#' the MI-LASSO regression on the full dataset and return the \code{grplasso} model.
#' 
#' To use \code{impgrplasso}, we must first generate a list of imputed data 
#' frames. This can be done for example using the \code{mice} function from the 
#' 'mice' package or using the function \code{mice_by_group} from this package. 
#' The list of data frames must have all categorical columns transformed into
#' dummy variables. This can either be done beforehand or the user can set the
#' argument 'dummify' to TRUE (the default), in which case the function will do
#' that transformation. Note that for proper functioning, variable names should
#' not contain underscores ("_") so that when the dummy variables are generated,
#' an underscore can be added by \code{impgrplasso} to separate the variable 
#' name (before the added underscore) from its respective levels (after the added
#' underscore). 
#' 
#' @param impdatlist A list of data frames generated by multiple imputation. 
#' For example, one could obtain a 'mice' package-derived 'mids' class object 
#' with \code{imp <- mice(nhanes)} and then obtain from that a list of imputed 
#' data frames with \code{dlist <- lapply(1:imp$m, function(x) complete(imp, x))}. 
#' @param lams Either a single lambda value or a vector of lambda values. If one
#' value is provided, the function will perform the MI-LASSO on the full dataset
#' using only that single lambda value. If a vector of values is provided, the
#' function will perform cross-validation to obtain fold- and imputation-specific
#' as well as averaged loss values and coefficients for each lambda value so 
#' that an optimal lambda can be chosen.
#' @param outname A column name for the dependent variable. This can be continuous
#' or binary. In the case of the latter, a LASSO logistic regression will be 
#' performed.
#' @param prednames Column names for the independent variables. These should be
#' free of underscores. If dummy variables have already been made for the list
#' of data frames \code{impdatlist}, provide those names with underscores between
#' the variable name and the level (e.g., "sex_male"). 
#' @param forcedin This is an optional vector of column names that should be a 
#' subset of \code{prednames} that identifies unpenalized variables to be
#' "forced in" to the model due to them not experiencing any shrinkage.
#' @param kfolds If a vector of lambda values is provided for the argument 
#' \code{lams}, \code{kfolds} provides the number of cross-validation folds for
#' the cross-validation process. This defaults to 10.
#' @param kfoldseed The rows chosen for each k-fold cross-validation are randomly
#' selected once at the beginning of the function and then this selection is used
#' consistently throughout subsequent steps. This value sets a seed for that 
#' random process. This defaults to 10.
#' @param scalecenter If TRUE (default), the function will scale and center all
#' variables prior to calculations by subtracting the means of each variable and 
#' dividing by the standard deviations. This is recommended for LASSO regression,
#' but one can also perform their own form of standardization in the list of data
#' frames and set this to FALSE if they prefer another standardization approach.
#' @param dummify If TRUE (default), the function will produce dummy variables
#' for all variables in \code{prednames} of classes 'factor' or 'character'. This
#' can be set to FALSE if one wants to create dummy variables ahead of time and 
#' include them in the list of data frames \code{impdatlist}.
#' 
#' @return \code{impgrplasso} returns a list of class 'impgrplasso' which contains:
#' 
#' \item{Coef}{If \code{lams} is a vector, this is a list of fold-specific data 
#' frames containing coefficients for each variable for specific imputed 
#' datasets and lambda values. If \code{lams} is a single value, this is a 
#' single data frame with variable- and imputation-specific coefficients.}
#' \item{MeanCoef}{If \code{lams} is a vector of length > 1, this is a data 
#' frame of mean coefficients across imputed datasets for each variable at 
#' each fold and lambda value. If \code{lams} is a single value, this is a data 
#' frame with mean coefficients across imputed datasets for each variable.}
#' \item{Loss}{This appears only if \code{lams} is a vector of length > 1, and 
#' it is a data frame showing fold- and imputation-specific loss values and 
#' means for each value of lambda.}
#' \item{MeanLoss}{This appears only if \code{lams} is a vector of length > 1, 
#' and it is a data frame showing the mean prediction loss for each lambda value.}
#' \item{Model}{This appears only if \code{lams} is a single value, and it is
#' the \code{grplasso} model object run on the full dataset.}
#' \item{Index}{This is the \code{index} argument for the \code{grplasso} 
#' function generated by \code{impgrplasso}.}
#' \item{allX}{This appears only if \code{lams} is a single value, and it is
#' the large stacked matrix of independent variables across imputed datasets 
#' generated by \code{impgrplasso}.}
#' 
#' @import grplasso
#' @import fastDummies
#' @export impgrplasso
#' 
#' @examples 
#' nhanes$hyp <- as.factor(nhanes$hyp - 1)
#' # above brings "hyp" from a {1, 2} set of unique values to {0, 1}
#' imp <- mice(nhanes)
#' dlist <- lapply(1:imp$m, function(x) complete(imp, x))
#' 
#' singlerun_milasso <- impgrplasso(dlist, 5, "hyp", c("age", "bmi", "chl"),
#' "age")
#' # This forces "age" into the model by not penalizing that variable.
#' 
#' summary(singlerun_milasso)
#' 
#' @references 
#' 
#' Chen, Q., & Wang, S. (2013). Variable selection for multiply‐imputed data 
#' with application to dioxin exposure study. Statistics in medicine, 32(21), 
#' 3646-3659.
#' 
#' Gunn, H. J., Hayati Rezvan P., Fernandez M. I., Comulada W. S. (2023). How 
#' to apply variable selection machine learning algorithms with multiply 
#' imputed data: A missing discussion. Psychological methods, 28(2), 452.
#' 
#' Meier, L., Van De Geer, S., & Bühlmann, P. (2008). The group lasso for 
#' logistic regression. Journal of the Royal Statistical Society Series B: 
#' Statistical Methodology, 70(1), 53-71.
#' 
impgrplasso <- function(impdatlist, lams, outname, prednames = NULL,
                        forcedin = NULL, kfolds = 10, kfoldseed = 10, 
                        scalecenter = TRUE, dummify = TRUE){
  if(is.integer(outname)|is.numeric(outname)){
    outname <- names(impdatlist[[1]])[outname]
  }
  if(is.null(prednames)) prednames <- setdiff(names(impdatlist[[1]]), outname)
  if(is.integer(prednames)|is.numeric(prednames)){
    prednames <- names(longimpdat)[prednames]
  }
  
  for(i in 1:length(impdatlist)) {impdatlist[[i]]<-
    impdatlist[[i]][,c(outname,prednames)]}
  
  if(class(impdatlist[[1]][,outname])%in%c("factor","character")&
     length(unique(impdatlist[[1]][,outname]))!=2){
    stop("Error: 'outname' column must be binary or numeric.")}
  
  if(dummify){
    faccharcols<-names(impdatlist[[1]])[
      sapply(impdatlist[[1]], 
             function(x) class(x)=="character"|class(x)=="factor")]
    impdatlist <- lapply(impdatlist,
                         function(x) fastDummies::dummy_cols(
                           x, select_columns = faccharcols,
                           ignore_na = T, remove_selected_columns = T,
                           remove_first_dummy = T))
  }
  
  longimpdat<-do.call("rbind", impdatlist)
  Impvec <- rep(1:length(impdatlist), each = nrow(impdatlist[[1]]))
  outcol<-names(longimpdat)[grep(outname, names(longimpdat))]
  if(length(outcol) > 1) 
    stop("Error: 'outname' must indicate one unique column that is either binary or numeric")
  
  names(longimpdat) <- gsub("<|>|\\.|,|\\$|\\'| |-|\\(|\\)","",
                            names(longimpdat))
  
  yvec <- longimpdat[, outcol]
  diagimpdat <- longimpdat[, -which(names(longimpdat)==outcol)]
  diagimpdat[which(Impvec!=1),] <- 0
  colnames(diagimpdat) <- paste0(colnames(diagimpdat), "|1")
  for(i in 2:length(impdatlist)){
    adduct <- as.matrix(longimpdat)
    adduct[which(Impvec!=i),]<-0
    colnames(adduct) <- paste0(colnames(adduct), "|", i)
    diagimpdat <- cbind(diagimpdat, adduct)
  }
  
  if(!is.null(kfolds)){
    fold <- cut(seq(1, nrow(impdatlist[[1]])), breaks = kfolds, labels = FALSE)
    set.seed(kfoldseed)
    fold <- sample(fold, length(fold), replace = FALSE)
    folds <- rep(fold, times = length(impdatlist))
  }
  
  allmilcoef <- list()
  grpl_index <- rep(NA, length(prednames))
  if(!is.null(forcedin)){
    penpreds <- setdiff(prednames, forcedin)
  } else {
    penpreds <- prednames
  }
  
  penpredprefix <- gsub("_.*","",penpreds)
  penpredindex <- match(penpredprefix, unique(penpredprefix))
  grpl_index[match(penpreds, prednames)] <- penpredindex
  grpl_index <- c(rep(NA, length(impdatlist)), rep(grpl_index, length(impdatlist)))
  
  if(length(unique(yvec)) == 2){
    LossFunc <- function(true, pred){
      -1/length(true) * sum(true * log(pred) + (1-true) * log(1-pred))
    }
  } else {
    LossFunc <- function(true, pred){(1/length(true)) * sum((true - pred)^2)}
  }
  
  losslist <- list()
  
  impbeta0mat <- matrix(0, nrow(longimpdat), length(impdatlist) + 1)
  impbeta0mat[, 1] <- rep(1:length(impdatlist), 
                          each = nrow(longimpdat)/length(impdatlist))
  for(i in 1:length(impdatlist)){
    impbeta0mat[which(impbeta0mat[, 1] == i), i + 1] <- 1
  }
  impbeta0mat <- impbeta0mat[, -1]
  colnames(impbeta0mat) <- paste0("beta0|", 1:ncol(impbeta0mat))
  
  if(length(lams) > 1){
    for(ff in 1:kfolds){
      print(paste0("Fold ",ff))
      longimpdatfold <- longimpdat[which(folds != ff),]
      if(scalecenter){
        longstd <- scale(longimpdatfold[, -which(names(longimpdatfold)==outcol)])
        longstdterms <- list(Centers = attr(longstd, "scaled:center"),
                             Scales = attr(longstd, "scaled:scale"))
        diagimpdat.std <- longstd
      } else {
        diagimpdat.std <- longimpdatfold[, -which(names(longimpdatfold)==outcol)]
      }
      
      foldimpvec <- Impvec[which(folds != ff)]
      diagimpdat.std[which(foldimpvec != 1),] <- 0
      colnames(diagimpdat.std) <- paste0(colnames(diagimpdat.std), "|1")
      for(i in 2:length(impdatlist)){
        adduct <- as.matrix(longstd)
        adduct[which(foldimpvec != i),] <- 0
        colnames(adduct) <- paste0(colnames(adduct), "|", i)
        diagimpdat.std <- cbind(diagimpdat.std, adduct)
      }
      allX <- cbind(impbeta0mat[which(folds != ff),],
                    as.matrix(diagimpdat.std))
      losslist[[ff]] <- as.data.frame(matrix(0, length(lams), 
                                             length(impdatlist) + 1))
      allmilcoef[[ff]] <- as.data.frame(matrix(0,0,
                                               length(impdatlist) + 3))
      names(allmilcoef[[ff]]) <- 
        c("Variable", paste0("Imp", 1:length(impdatlist)),"Fold","Lambda")
      tmpyorig <- yvec[-which(folds == ff)]
      
      for(l in 1:length(lams)){
        tempmil <- suppressWarnings(grplasso(x = allX, y = tmpyorig,
                            index = grpl_index, lambda = lams[l],
                            center = FALSE, standardize = FALSE))
        coefdat <- as.data.frame(tempmil$coefficients)
        names(coefdat)[1] <- "Coef"
        coefdat$VarName <- gsub("\\|.*", "", rownames(tempmil$coefficients))
        coefdat$Imp <- gsub(".*\\|", "", rownames(tempmil$coefficients))
        if(!any(is.na(as.integer(coefdat$Imp)))) 
          coefdat$Imp <- as.integer(coefdat$Imp)
        
        tempmil$coefficients <- 
          reshape(coefdat, v.names = "Coef", timevar = "Imp",
                  idvar = "VarName", direction = "wide")
        names(tempmil$coefficients) <- c("Variable",
                                         paste0("Imp", 1:length(impdatlist)))
        
        #Rescale coef
        if(scalecenter){
          for(i in 2:ncol(tempmil$coefficients)){
            tempmil$coefficients[-1, i] <- tempmil$coefficients[-1, i]/
              longstdterms$Scales
            tempmil$coefficients[1, i] <- mean(tmpyorig) - 
              tempmil$coefficients[-1, i] %*% longstdterms$Centers
          }
        }
        
        tempmil$coefficients$Fold <- ff
        tempmil$coefficients$Lambda <- lams[l]
        allmilcoef[[ff]] <- rbind(allmilcoef[[ff]], tempmil$coefficients)
        
        mseMIL = matrix(NA, length(impdatlist))
        
        #Calculate predicted scores based on model in validation fold, for
        # each imputed data set
        for(d in 1:length(impdatlist)){
          valdat <- impdatlist[[d]]
          names(valdat) <- gsub("<|>|\\.|,|\\$|\\'| |-|\\(|\\)","",
                                names(valdat))
          xd <- as.matrix(valdat[which(fold == ff), -which(names(valdat)==outcol)])
          yd <- valdat[which(fold == ff), outcol]
          b0d <- tempmil$coefficients[1, paste0("Imp", d)]
          bd <- tempmil$coefficients[2:nrow(tempmil$coefficients),
                                     paste0("Imp", d)]
          linpred <- xd %*% bd + b0d
          if(length(unique(yvec)) == 2){
            predictVal <- 1/(1+exp(-linpred))
          } else {
            predictVal <- linpred
          }
          mseMIL[d] <- LossFunc(yd, predictVal)
        }
        
        losslist[[ff]][l, 1] <- lams[l]
        losslist[[ff]][l, -1] <- mseMIL
      }
    }
    allloss <- do.call("rbind", losslist)
    names(allloss) <- c("Lambda", paste0("Imp", 1:length(impdatlist)))
    allloss$MeanLoss <- rowMeans(allloss[, -1])
    allloss$Fold <- rep(1:kfolds, each = length(lams))
    allloss <- allloss[, c(ncol(allloss), 1:(ncol(allloss) - 1))]
    MeanCoef <- do.call("rbind", allmilcoef)
    MeanCoef$Variable <- factor(MeanCoef$Variable,
                                levels = unique(allmilcoef[[1]]$Variable))
    MeanCoef$Mean <- rowMeans(MeanCoef[,2:(length(impdatlist) + 1)])
    MeanCoef <- aggregate(Mean ~ Variable + Fold + Lambda, MeanCoef, mean)
    
    meanloss<-aggregate(MeanLoss ~ Lambda, allloss, mean)
    retlist <- list(Coef = allmilcoef, MeanCoef = MeanCoef, Loss = allloss, 
                    MeanLoss = meanloss, Index = grpl_index)
  } else {
    if(scalecenter){
      longstd <- scale(longimpdat[, prednames])
      longstdterms <- list(Centers = attr(longstd, "scaled:center"),
                           Scales = attr(longstd, "scaled:scale"))
      diagimpdat.std <- longstd
    } else {
      diagimpdat.std <- longimpdat[, prednames]
    }
    diagimpdat.std[which(Impvec != 1),] <- 0
    colnames(diagimpdat.std) <- paste0(colnames(diagimpdat.std), "|1")
    for(i in 2:length(impdatlist)){
      adduct <- as.matrix(longstd)
      adduct[which(Impvec != i),] <- 0
      colnames(adduct) <- paste0(colnames(adduct), "|", i)
      diagimpdat.std <- cbind(diagimpdat.std, adduct)
    }
    
    allX <- cbind(impbeta0mat, as.matrix(diagimpdat.std))
    
    tempmil <- suppressWarnings(grplasso(x = allX, y = yvec, index = grpl_index,
                        lambda = lams[1], center = FALSE, standardize = FALSE))
    coefdat <- as.data.frame(tempmil$coefficients)
    names(coefdat)[1] <- "Coef"
    coefdat$VarName <- gsub("\\|.*", "", rownames(tempmil$coefficients))
    coefdat$Imp <- gsub(".*\\|", "", rownames(tempmil$coefficients))
    if(!any(is.na(as.integer(coefdat$Imp)))) coefdat$Imp <- as.integer(coefdat$Imp)
    tempmil$coefficients <- reshape(coefdat, v.names = "Coef", timevar = "Imp",
                                    idvar = "VarName", direction = "wide")
    names(tempmil$coefficients) <- c("Variable", paste0("Imp", 1:length(impdatlist)))
    
    #Rescale coefficients
    if(scalecenter){
      for(i in 2:ncol(tempmil$coefficients)){
        tempmil$coefficients[-1, i] <- tempmil$coefficients[-1, i]/
          longstdterms$Scales
        tempmil$coefficients[1, i] <- mean(yvec) - 
          tempmil$coefficients[-1, i] %*% longstdterms$Centers
      }
    }
    tempmil$coefficients$MeanCoef<-rowMeans(tempmil$coefficients[,-1])
    MeanCoef<-tempmil$coefficients$MeanCoef
    names(MeanCoef)<-tempmil$coefficients$Variable
    retlist <- list(Coef = tempmil$coefficients, MeanCoef = MeanCoef, 
                    Model = tempmil, Index = grpl_index, allX = allX)
  }
  class(retlist)<-"impgrplasso"
  return(retlist)
}

#' @rawNamespace S3method(summary, impgrplasso)

summary.impgrplasso <- function(object, ...){
  if(any(grepl("MeanLoss",names(object)))){
    lossout<-object$MeanLoss$MeanLoss
    names(lossout)<-object$MeanLoss$Lambda
    message("Mean prediction loss by lambda: \n")
    lossout
  } else {
    message("Mean LASSO regression coefficients: \n")
    object$MeanCoef
  }
}
